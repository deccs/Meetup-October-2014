{
 "metadata": {
  "name": "",
  "signature": "sha256:e1cf49c0308deac221b64ff22459e9b45cf8af16ec6beca3f31d95ae8499101e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Theano Neural Network Training for Kaggle Titanic Data\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import required models\n",
      "import numpy as np\n",
      "import theano\n",
      "# By convention, the tensor submodule is loaded as T\n",
      "import theano.tensor as T\n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define a class Layer. A neural network consists of one or more computational layers.\n",
      "class Layer(object):\n",
      "    def __init__(self, W_init, b_init, activation):\n",
      "        '''\n",
      "        A layer of a neural network, computes s(Wx + b) where s is a nonlinearity and x is the input vector.\n",
      "\n",
      "        :parameters:\n",
      "            - W_init : np.ndarray, shape=(n_output, n_input)\n",
      "                Values to initialize the weight matrix to.\n",
      "            - b_init : np.ndarray, shape=(n_output,)\n",
      "                Values to initialize the bias vector\n",
      "            - activation : theano.tensor.elemwise.Elemwise\n",
      "                Activation function for layer output\n",
      "        '''\n",
      "        # Retrieve the input and output dimensionality based on W's initialization\n",
      "        n_output, n_input = W_init.shape\n",
      "        # Make sure b is n_output in size\n",
      "        assert b_init.shape == (n_output,)\n",
      "        # All parameters should be shared variables.\n",
      "        # They're used in this class to compute the layer output,\n",
      "        # but are updated elsewhere when optimizing the network parameters.\n",
      "        # Note that we are explicitly requiring that W_init has the theano.config.floatX dtype\n",
      "        self.W = theano.shared(value=W_init.astype(theano.config.floatX),\n",
      "                               # The name parameter is solely for printing purporses\n",
      "                               name='W',\n",
      "                               # Setting borrow=True allows Theano to use user memory for this object.\n",
      "                               # It can make code slightly faster by avoiding a deep copy on construction.\n",
      "                               # For more details, see\n",
      "                               # http://deeplearning.net/software/theano/tutorial/aliasing.html\n",
      "                               borrow=True)\n",
      "        # We can force our bias vector b to be a column vector using numpy's reshape method.\n",
      "        # When b is a column vector, we can pass a matrix-shaped input to the layer\n",
      "        # and get a matrix-shaped output, thanks to broadcasting (described below)\n",
      "        self.b = theano.shared(value=b_init.reshape(-1, 1).astype(theano.config.floatX),\n",
      "                               name='b',\n",
      "                               borrow=True,\n",
      "                               # Theano allows for broadcasting, similar to numpy.\n",
      "                               # However, you need to explicitly denote which axes can be broadcasted.\n",
      "                               # By setting broadcastable=(False, True), we are denoting that b\n",
      "                               # can be broadcast (copied) along its second dimension in order to be\n",
      "                               # added to another variable.  For more information, see\n",
      "                               # http://deeplearning.net/software/theano/library/tensor/basic.html\n",
      "                               broadcastable=(False, True))\n",
      "        self.activation = activation\n",
      "        # We'll compute the gradient of the cost of the network with respect to the parameters in this list.\n",
      "        self.params = [self.W, self.b]\n",
      "        \n",
      "    def output(self, x):\n",
      "        '''\n",
      "        Compute this layer's output given an input\n",
      "        \n",
      "        :parameters:\n",
      "            - x : theano.tensor.var.TensorVariable\n",
      "                Theano symbolic variable for layer input\n",
      "\n",
      "        :returns:\n",
      "            - output : theano.tensor.var.TensorVariable\n",
      "                Mixed, biased, and activated x\n",
      "        '''\n",
      "        # Compute linear mix\n",
      "        lin_output = T.dot(self.W, x) + self.b\n",
      "        # Output is just linear mix if no activation function\n",
      "        # Otherwise, apply the activation function\n",
      "        return (lin_output if self.activation is None else self.activation(lin_output))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP(object):\n",
      "    def __init__(self, W_init, b_init, activations):\n",
      "        '''\n",
      "        Multi-layer perceptron class, computes the composition of a sequence of Layers\n",
      "\n",
      "        :parameters:\n",
      "            - W_init : list of np.ndarray, len=N\n",
      "                Values to initialize the weight matrix in each layer to.\n",
      "                The layer sizes will be inferred from the shape of each matrix in W_init\n",
      "            - b_init : list of np.ndarray, len=N\n",
      "                Values to initialize the bias vector in each layer to\n",
      "            - activations : list of theano.tensor.elemwise.Elemwise, len=N\n",
      "                Activation function for layer output for each layer\n",
      "        '''\n",
      "        # Make sure the input lists are all of the same length\n",
      "        assert len(W_init) == len(b_init) == len(activations)\n",
      "        \n",
      "        # Initialize lists of layers\n",
      "        self.layers = []\n",
      "        # Construct the layers\n",
      "        for W, b, activation in zip(W_init, b_init, activations):\n",
      "            self.layers.append(Layer(W, b, activation))\n",
      "\n",
      "        # Combine parameters from all layers\n",
      "        self.params = []\n",
      "        for layer in self.layers:\n",
      "            self.params += layer.params\n",
      "        \n",
      "    def output(self, x):\n",
      "        '''\n",
      "        Compute the MLP's output given an input\n",
      "        \n",
      "        :parameters:\n",
      "            - x : theano.tensor.var.TensorVariable\n",
      "                Theano symbolic variable for network input\n",
      "\n",
      "        :returns:\n",
      "            - output : theano.tensor.var.TensorVariable\n",
      "                x passed through the MLP\n",
      "        '''\n",
      "        # Recursively compute output\n",
      "        for layer in self.layers:\n",
      "            x = layer.output(x)\n",
      "        return x\n",
      "\n",
      "    def squared_error(self, x, y):\n",
      "        '''\n",
      "        Compute the squared euclidean error of the network output against the \"true\" output y\n",
      "        \n",
      "        :parameters:\n",
      "            - x : theano.tensor.var.TensorVariable\n",
      "                Theano symbolic variable for network input\n",
      "            - y : theano.tensor.var.TensorVariable\n",
      "                Theano symbolic variable for desired network output\n",
      "\n",
      "        :returns:\n",
      "            - error : theano.tensor.var.TensorVariable\n",
      "                The squared Euclidian distance between the network output and y\n",
      "        '''\n",
      "        return T.sum((self.output(x) - y)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_updates_momentum(cost, params, learning_rate, momentum):\n",
      "    '''\n",
      "    Compute updates for gradient descent with momentum\n",
      "    \n",
      "    :parameters:\n",
      "        - cost : theano.tensor.var.TensorVariable\n",
      "            Theano cost function to minimize\n",
      "        - params : list of theano.tensor.var.TensorVariable\n",
      "            Parameters to compute gradient against\n",
      "        - learning_rate : float\n",
      "            Gradient descent learning rate\n",
      "        - momentum : float\n",
      "            Momentum parameter, should be at least 0 (standard gradient descent) and less than 1\n",
      "   \n",
      "    :returns:\n",
      "        updates : list\n",
      "            List of updates, one for each parameter\n",
      "    '''\n",
      "    # Make sure momentum is a sane value\n",
      "    assert momentum < 1 and momentum >= 0\n",
      "    # List of update steps for each parameter\n",
      "    updates = []\n",
      "    # Just gradient descent on cost\n",
      "    for param in params:\n",
      "        # For each parameter, we'll create a param_update shared variable.\n",
      "        # This variable will keep track of the parameter's update step across iterations.\n",
      "        # We initialize it to 0\n",
      "        param_update = theano.shared(param.get_value()*0., broadcastable=param.broadcastable)\n",
      "        # Each parameter is updated by taking a step in the direction of the gradient.\n",
      "        # However, we also \"mix in\" the previous step according to the given momentum value.\n",
      "        # Note that when updating param_update, we are using its old value and also the new gradient step.\n",
      "        updates.append((param, param - learning_rate*param_update))\n",
      "        # Note that we don't need to derive backpropagation to compute updates - just use T.grad!\n",
      "        updates.append((param_update, momentum*param_update + (1. - momentum)*T.grad(cost, param)))\n",
      "    return updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training data - Load Kaggle Titanic Data from Notebook 1\n",
      "train_data=np.load(\"titanic_train.npy\")\n",
      "train_supervision=np.load(\"train_targets.npy\")\n",
      "inputSize = train_data.T.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define network topology\n",
      "layer_sizes = [inputSize, inputSize*2, 1]\n",
      "layer_sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[4, 8, 1]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, set the size of each layer (and the number of layers)\n",
      "# Input layer size is training data dimensionality (2)\n",
      "# Output size is just 1-d: class label - 0 or 1\n",
      "# Set initial parameter values\n",
      "W_init = []\n",
      "b_init = []\n",
      "activations = []\n",
      "for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
      "    # Getting the correct initialization matters a lot for non-toy problems.\n",
      "    # However, here we can just use the following initialization with success:\n",
      "    # Normally distribute initial weights\n",
      "    W_init.append(np.random.randn(n_output, n_input))\n",
      "    # Set initial biases to 1\n",
      "    b_init.append(np.ones(n_output))\n",
      "    # We'll use sigmoid activation for all layers\n",
      "    # Note that this doesn't make a ton of sense when using squared distance\n",
      "    # because the sigmoid function is bounded on [0, 1].\n",
      "    activations.append(T.nnet.sigmoid)\n",
      "# Create an instance of the MLP class\n",
      "mlp = MLP(W_init, b_init, activations)\n",
      "\n",
      "# Create Theano variables for the MLP input\n",
      "mlp_input = T.matrix('mlp_input')\n",
      "# ... and the desired output\n",
      "mlp_target = T.vector('mlp_target')\n",
      "# Learning rate and momentum hyperparameter values\n",
      "# Again, for non-toy problems these values can make a big difference\n",
      "# as to whether the network (quickly) converges on a good local minimum.\n",
      "learning_rate = 0.01\n",
      "momentum = 0.9\n",
      "# Create a function for computing the cost of the network given an input\n",
      "cost = mlp.squared_error(mlp_input, mlp_target)\n",
      "# Create a theano function for training the network\n",
      "train = theano.function([mlp_input, mlp_target], cost,\n",
      "                        updates=gradient_updates_momentum(cost, mlp.params, learning_rate, momentum))\n",
      "# Create a theano function for computing the MLP's output given some input\n",
      "mlp_output = theano.function([mlp_input], mlp.output(mlp_input))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create function to sample mini-batches from the entire training set\n",
      "#To be used with itertools\n",
      "def sampleMiniBatch(mbsz, inps, targs):\n",
      "    idx = np.random.randint(inps.shape[0],size=(mbsz,))\n",
      "    return inps[idx], targs[idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mbsz=20\n",
      "mbPerEpoch = int(np.ceil(train_data.shape[0]/mbsz))\n",
      "print \"Number of mini-batches per epoch:%d\"%mbPerEpoch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of mini-batches per epoch:44\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define iterator to sample training data into mini-batches for stochastic gradient descent\n",
      "mbStream= (sampleMiniBatch(mbsz, train_data, train_supervision) for unused in itertools.repeat(None))\n",
      "\n",
      "# Keep track of the number of training iterations performed\n",
      "iteration = 0\n",
      "# We'll only train the network within 20 iterations.\n",
      "# A more common technique is to use a hold-out validation set.\n",
      "# When the validation error starts to increase, the network is overfitting,\n",
      "# so we stop training the net.  This is called \"early stopping\", which we won't do here.\n",
      "max_iteration = 13\n",
      "\n",
      "while iteration < max_iteration:\n",
      "    accuracy = 0\n",
      "    cost = 0\n",
      "    totalCases = 0\n",
      "    # Train the network using the entire training set.\n",
      "    # Train here with mini-batch stochastic gradient descent    \n",
      "    for i in range(mbPerEpoch):    \n",
      "        X, y = mbStream.next()\n",
      "        current_cost = train(X.T, y)\n",
      "        # Get the current network output for all points in the training set\n",
      "        current_output = mlp_output(X.T)\n",
      "        # We can compute the accuracy by thresholding the output\n",
      "        # and computing the proportion of points whose class match the ground truth class.\n",
      "        accuracy += np.mean((current_output > .5) == y)*mbsz\n",
      "        cost += current_cost\n",
      "        totalCases += X.shape[0]    \n",
      "    \n",
      "    print(\"Iteration:%d Cost:%f, Accuracy:%f\"%(iteration,current_cost/totalCases,(accuracy/totalCases)*100))    \n",
      "    iteration += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration:0 Cost:0.004685, Accuracy:43.977273\n",
        "Iteration:1 Cost:0.005136, Accuracy:61.363636\n",
        "Iteration:2 Cost:0.005112, Accuracy:61.363636\n",
        "Iteration:3 Cost:0.004348, Accuracy:63.409091\n",
        "Iteration:4 Cost:0.004485, Accuracy:64.090909\n",
        "Iteration:5 Cost:0.004651, Accuracy:65.113636"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration:6 Cost:0.005317, Accuracy:66.818182\n",
        "Iteration:7 Cost:0.005019, Accuracy:70.795455\n",
        "Iteration:8 Cost:0.004532, Accuracy:72.045455\n",
        "Iteration:9 Cost:0.005195, Accuracy:68.295455\n",
        "Iteration:10 Cost:0.003410, Accuracy:71.818182"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration:11 Cost:0.003544, Accuracy:72.272727\n",
        "Iteration:12 Cost:0.003201, Accuracy:79.318182\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load test data and predict\n",
      "testData=np.load(\"titanic_test.npy\")\n",
      "output=mlp_output(testData.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Prepare kaggle submission file\n",
      "testPassengerStartIndex=892\n",
      "outputs=[[x[0]+testPassengerStartIndex,x[1]] for x in enumerate((mlp_output(testData.T) > 0.5).astype('int')[0])]\n",
      "np.savetxt('data/submission_th_2.csv', outputs, delimiter=',', fmt='%d,%d', \n",
      "            header='PassengerId,Survived', comments = '')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Score on Kaggle: 0.77512\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}